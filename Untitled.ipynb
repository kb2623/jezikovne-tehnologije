{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namesti:\n",
    "* python-nltk\n",
    "* python-scikit-learn\n",
    "\n",
    "Pomoc:\n",
    "* [Knjiga](http://www.nltk.org/book_1ed/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "Hit Enter to continue: \n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "Hit Enter to continue: \n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "Hit Enter to continue: \n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n",
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package biocreative_ppi to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package brown to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /home/klemen/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/comparative_sentences.zip.\n",
      "       | Downloading package comtrans to /home/klemen/nltk_data...\n",
      "       | Downloading package conll2000 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to /home/klemen/nltk_data...\n",
      "       | Downloading package crubadan to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package dolch to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/dolch.zip.\n",
      "       | Downloading package europarl_raw to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/europarl_raw.zip.\n",
      "       | Downloading package floresta to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package framenet_v17 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v17.zip.\n",
      "       | Downloading package gazetteers to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /home/klemen/nltk_data...\n",
      "       | Downloading package kimmo to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /home/klemen/nltk_data...\n",
      "       | Downloading package lin_thesaurus to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip.\n",
      "       | Downloading package mac_morpho to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/mac_morpho.zip.\n",
      "       | Downloading package machado to /home/klemen/nltk_data...\n",
      "       | Downloading package masc_tagged to /home/klemen/nltk_data...\n",
      "       | Downloading package moses_sample to /home/klemen/nltk_data...\n",
      "       |   Unzipping models/moses_sample.zip.\n",
      "       | Downloading package movie_reviews to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/movie_reviews.zip.\n",
      "       | Downloading package names to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/names.zip.\n",
      "       | Downloading package nombank.1.0 to /home/klemen/nltk_data...\n",
      "       | Downloading package nps_chat to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/nps_chat.zip.\n",
      "       | Downloading package omw to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/omw.zip.\n",
      "       | Downloading package opinion_lexicon to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/opinion_lexicon.zip.\n",
      "       | Downloading package paradigms to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/paradigms.zip.\n",
      "       | Downloading package pil to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/pil.zip.\n",
      "       | Downloading package pl196x to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/pl196x.zip.\n",
      "       | Downloading package ppattach to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/ppattach.zip.\n",
      "       | Downloading package problem_reports to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/problem_reports.zip.\n",
      "       | Downloading package propbank to /home/klemen/nltk_data...\n",
      "       | Downloading package ptb to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/ptb.zip.\n",
      "       | Downloading package product_reviews_1 to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_1.zip.\n",
      "       | Downloading package product_reviews_2 to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_2.zip.\n",
      "       | Downloading package pros_cons to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/pros_cons.zip.\n",
      "       | Downloading package qc to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/qc.zip.\n",
      "       | Downloading package reuters to /home/klemen/nltk_data...\n",
      "       | Downloading package rte to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/rte.zip.\n",
      "       | Downloading package semcor to /home/klemen/nltk_data...\n",
      "       | Downloading package senseval to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/senseval.zip.\n",
      "       | Downloading package sentiwordnet to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/sentiwordnet.zip.\n",
      "       | Downloading package sentence_polarity to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/sentence_polarity.zip.\n",
      "       | Downloading package shakespeare to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/shakespeare.zip.\n",
      "       | Downloading package sinica_treebank to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/sinica_treebank.zip.\n",
      "       | Downloading package smultron to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/smultron.zip.\n",
      "       | Downloading package state_union to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/state_union.zip.\n",
      "       | Downloading package stopwords to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/stopwords.zip.\n",
      "       | Downloading package subjectivity to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/subjectivity.zip.\n",
      "       | Downloading package swadesh to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/swadesh.zip.\n",
      "       | Downloading package switchboard to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/switchboard.zip.\n",
      "       | Downloading package timit to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/timit.zip.\n",
      "       | Downloading package toolbox to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/toolbox.zip.\n",
      "       | Downloading package treebank to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/treebank.zip.\n",
      "       | Downloading package twitter_samples to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/twitter_samples.zip.\n",
      "       | Downloading package udhr to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/udhr.zip.\n",
      "       | Downloading package udhr2 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/udhr2.zip.\n",
      "       | Downloading package unicode_samples to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/unicode_samples.zip.\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       | Downloading package verbnet to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/verbnet.zip.\n",
      "       | Downloading package webtext to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/webtext.zip.\n",
      "       | Downloading package wordnet to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/wordnet.zip.\n",
      "       | Downloading package wordnet_ic to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/wordnet_ic.zip.\n",
      "       | Downloading package words to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/words.zip.\n",
      "       | Downloading package ycoe to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/ycoe.zip.\n",
      "       | Downloading package rslp to /home/klemen/nltk_data...\n",
      "       |   Unzipping stemmers/rslp.zip.\n",
      "       | Downloading package hmm_treebank_pos_tagger to\n",
      "       |     /home/klemen/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |   Unzipping taggers/hmm_treebank_pos_tagger.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package universal_tagset to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping taggers/universal_tagset.zip.\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "       | Downloading package punkt to /home/klemen/nltk_data...\n",
      "       |   Unzipping tokenizers/punkt.zip.\n",
      "       | Downloading package book_grammars to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping grammars/book_grammars.zip.\n",
      "       | Downloading package sample_grammars to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping grammars/sample_grammars.zip.\n",
      "       | Downloading package spanish_grammars to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping grammars/spanish_grammars.zip.\n",
      "       | Downloading package basque_grammars to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping grammars/basque_grammars.zip.\n",
      "       | Downloading package large_grammars to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping grammars/large_grammars.zip.\n",
      "       | Downloading package tagsets to /home/klemen/nltk_data...\n",
      "       |   Unzipping help/tagsets.zip.\n",
      "       | Downloading package snowball_data to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       | Downloading package bllip_wsj_no_aux to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "       | Downloading package word2vec_sample to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping models/word2vec_sample.zip.\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       | Downloading package mte_teip5 to /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/mte_teip5.zip.\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "       | Downloading package perluniprops to /home/klemen/nltk_data...\n",
      "       |   Unzipping misc/perluniprops.zip.\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "       | Downloading package vader_lexicon to\n",
      "       |     /home/klemen/nltk_data...\n",
      "       | Downloading package porter_test to /home/klemen/nltk_data...\n",
      "       |   Unzipping stemmers/porter_test.zip.\n",
      "       | Downloading package wmt15_eval to /home/klemen/nltk_data...\n",
      "       |   Unzipping models/wmt15_eval.zip.\n",
      "       | Downloading package mwa_ppdb to /home/klemen/nltk_data...\n",
      "       |   Unzipping misc/mwa_ppdb.zip.\n",
      "       | \n",
      "     Done downloading collection all\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delitev na stavke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delitev na besede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrami nad besedami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "string = \"I really like python, it's pretty awesome.\"\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 3\n",
    "ngrams = ngrams(sentence.split(), n)\n",
    "for grams in ngrams:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrami nad crkami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' He', 'Hel', 'ell', 'llo', 'lo ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2ngrams(text, n=3, exact=True):\n",
    "    return [\"\".join(j) for j in zip(*[text[i:] for i in range(n)])]\n",
    "\n",
    "word2ngrams(' Hello ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thi', 'his'], [], [], ['foo'], ['bar'], ['sen', 'ent', 'nte', 'ten', 'enc', 'nce', 'ces'], ['and'], [], ['wan', 'ant'], [], ['ngr', 'gra', 'ram', 'ami', 'miz', 'ize'], []]\n",
      "[['thi', 'his'], [], [], ['foo'], ['bar'], ['sen', 'ent', 'nte', 'ten', 'enc', 'nce', 'ces'], ['and'], [], ['wan', 'ant'], [], ['ngr', 'gra', 'ram', 'ami', 'miz', 'ize'], []]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def zip_ngrams(text, n=3, exact=True):\n",
    "    return [\"\".join(j) for j in zip(*[text[i:] for i in range(n)])]\n",
    "\n",
    "def nozip_ngrams(text, n=3):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "x = [zip_ngrams(w) for w in words]\n",
    "y = [nozip_ngrams(w) for w in words]\n",
    "\n",
    "print ('%s\\n%s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "## Simplified Part-of-Speech Tagset\n",
    "Tag | Meaning |\tExamples\n",
    "-- | -- | --\n",
    "ADJ | adjective | new, good, high, special, big, local\n",
    "ADV | adverb | really, already, still, early, now\n",
    "CNJ | conjunction | and, or, but, if, while, although\n",
    "DET | determiner | the, a, some, most, every, no\n",
    "EX | existential | there, there's\n",
    "FW | foreign word | dolce, ersatz, esprit, quo, maitre\n",
    "MOD | modal verb | will, can, would, may, must, should\n",
    "N | noun | year, home, costs, time, education\n",
    "NP | proper noun | Alison, Africa, April, Washington\n",
    "NUM | number | twenty-four, fourth, 1991, 14:24\n",
    "PRO | pronoun | he, their, her, its, my, I, us\n",
    "P | preposition | on, of, at, with, by, into, under\n",
    "TO | the word to | to\n",
    "UH | interjection | ah, bang, ha, whee, hmpf, oops\n",
    "V | verb | is, has, get, do, make, see, run\n",
    "VD | past tense | said, took, told, made, asked\n",
    "VG | present participle | making, going, playing, working\n",
    "VN | past participle | given, taken, begun, sung\n",
    "WH | wh determiner | who, which, when, what, where, how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04020979020979021 0.07357859531772576 0.03836930455635491 5720 299 230 22\n",
      "0.002972027972027972 0.0033444816053511705 0.002951484965873455 5720 299 17 1\n",
      "0.0008741258741258741 0.006688963210702341 0.0005534034311012728 5720 299 5 2\n",
      "0.0024475524475524478 0.016722408026755852 0.0016602102933038186 5720 299 14 5\n",
      "0.0013986013986013986 0.0033444816053511705 0.0012912746725696365 5720 299 8 1\n",
      "0.0034965034965034965 0.0033444816053511705 0.003504888396974728 5720 299 20 1\n",
      "0.017132867132867134 0.04013377926421405 0.01586423169156982 5720 299 98 12\n",
      "0.005244755244755245 0.006688963210702341 0.005165098690278546 5720 299 30 2\n",
      "0.005944055944055944 0.026755852842809364 0.004796163069544364 5720 299 34 8\n",
      "0.04493006993006993 0.043478260869565216 0.04501014572957019 5720 299 257 13\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.016258741258741258 0.0802675585284281 0.012728278915329275 5720 299 93 24\n",
      "0.001048951048951049 0.0033444816053511705 0.0009223390518354548 5720 299 6 1\n",
      "0.029545454545454545 0.006688963210702341 0.030806124331304186 5720 299 169 2\n",
      "0.0015734265734265735 0.0033444816053511705 0.0014757424829367274 5720 299 9 1\n",
      "0.0038461538461538464 0.006688963210702341 0.003689356207341819 5720 299 22 2\n",
      "0.003146853146853147 0.006688963210702341 0.002951484965873455 5720 299 18 2\n",
      "0.0022727272727272726 0.0033444816053511705 0.002213613724405091 5720 299 13 1\n",
      "0.01381118881118881 0.020066889632107024 0.013466150156797639 5720 299 79 6\n",
      "0.006118881118881119 0.0033444816053511705 0.006271905552481092 5720 299 35 1\n",
      "0.0026223776223776225 0.010033444816053512 0.002213613724405091 5720 299 15 3\n",
      "0.009965034965034964 0.006688963210702341 0.010145729570190002 5720 299 57 2\n",
      "0.0012237762237762239 0.016722408026755852 0.00036893562073418186 5720 299 7 5\n",
      "0.0015734265734265735 0.006688963210702341 0.0012912746725696365 5720 299 9 2\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.0012237762237762239 0.006688963210702341 0.0009223390518354548 5720 299 7 2\n",
      "0.00034965034965034965 0.006688963210702341 0.0 5720 299 2 2\n",
      "0.0005244755244755245 0.006688963210702341 0.00018446781036709093 5720 299 3 2\n",
      "0.004370629370629371 0.0033444816053511705 0.004427227448810182 5720 299 25 1\n",
      "0.003146853146853147 0.013377926421404682 0.002582549345139273 5720 299 18 4\n",
      "0.0006993006993006993 0.0033444816053511705 0.0005534034311012728 5720 299 4 1\n",
      "0.0013986013986013986 0.0033444816053511705 0.0012912746725696365 5720 299 8 1\n",
      "0.0022727272727272726 0.0033444816053511705 0.002213613724405091 5720 299 13 1\n",
      "0.0013986013986013986 0.0033444816053511705 0.0012912746725696365 5720 299 8 1\n",
      "0.0024475524475524478 0.0033444816053511705 0.002398081534772182 5720 299 14 1\n",
      "0.0024475524475524478 0.006688963210702341 0.002213613724405091 5720 299 14 2\n",
      "0.0006993006993006993 0.0033444816053511705 0.0005534034311012728 5720 299 4 1\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.0008741258741258741 0.006688963210702341 0.0005534034311012728 5720 299 5 2\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.013111888111888112 0.010033444816053512 0.013281682346430549 5720 299 75 3\n",
      "0.003146853146853147 0.0033444816053511705 0.003135952776240546 5720 299 18 1\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.001048951048951049 0.013377926421404682 0.00036893562073418186 5720 299 6 4\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.004370629370629371 0.0033444816053511705 0.004427227448810182 5720 299 25 1\n",
      "0.015559440559440559 0.0033444816053511705 0.016233167312304002 5720 299 89 1\n",
      "0.0008741258741258741 0.006688963210702341 0.0005534034311012728 5720 299 5 2\n",
      "0.0005244755244755245 0.0033444816053511705 0.00036893562073418186 5720 299 3 1\n",
      "0.00034965034965034965 0.0033444816053511705 0.00018446781036709093 5720 299 2 1\n",
      "0.0019230769230769232 0.0033444816053511705 0.0018446781036709095 5720 299 11 1\n",
      "0.00034965034965034965 0.0033444816053511705 0.00018446781036709093 5720 299 2 1\n",
      "0.0015734265734265735 0.0033444816053511705 0.0014757424829367274 5720 299 9 1\n",
      "0.00034965034965034965 0.0033444816053511705 0.00018446781036709093 5720 299 2 1\n",
      "0.001048951048951049 0.0033444816053511705 0.0009223390518354548 5720 299 6 1\n",
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print (tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\",\n",
       " 'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.',\n",
       " 'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.',\n",
       " '(Applause.)',\n",
       " 'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan.',\n",
       " '31, 2006.',\n",
       " \"White House photo by Eric DraperEvery time I'm invited to this rostrum, I'm humbled by the privilege, and mindful of the history we've seen together.\",\n",
       " 'We have gathered under this Capitol dome in moments of national mourning and national achievement.',\n",
       " 'We have served America through one of the most consequential periods of our history -- and it has been my honor to serve with you.',\n",
       " 'In a system of two parties, two chambers, and two elected branches, there will always be differences and debate.',\n",
       " 'But even tough debates can be conducted in a civil tone, and our differences cannot be allowed to harden into anger.',\n",
       " 'To confront the great issues before us, we must act in a spirit of goodwill and respect for one another -- and I will do my part.',\n",
       " 'Tonight the state of our Union is strong -- and together we will make it stronger.',\n",
       " '(Applause.)',\n",
       " 'In this decisive year, you and I will make choices that determine both the future and the character of our country.',\n",
       " 'We will choose to act confidently in pursuing the enemies of freedom -- or retreat from our duties in the hope of an easier life.',\n",
       " 'We will choose to build our prosperity by leading the world economy -- or shut ourselves off from trade and opportunity.',\n",
       " 'In a complex and challenging time, the road of isolationism and protectionism may seem broad and inviting -- yet it ends in danger and decline.',\n",
       " 'The only way to protect our people, the only way to secure the peace, the only way to control our destiny is by our leadership -- so the United States of America will continue to lead.',\n",
       " '(Applause.)',\n",
       " 'Abroad, our nation is committed to an historic, long-term goal -- we seek the end of tyranny in our world.',\n",
       " 'Some dismiss that goal as misguided idealism.',\n",
       " 'In reality, the future security of America depends on it.',\n",
       " 'On September the 11th, 2001, we found that problems originating in a failed and oppressive state 7,000 miles away could bring murder and destruction to our country.',\n",
       " 'Dictatorships shelter terrorists, and feed resentment and radicalism, and seek weapons of mass destruction.',\n",
       " 'Democracies replace resentment with hope, respect the rights of their citizens and their neighbors, and join the fight against terror.',\n",
       " \"Every step toward freedom in the world makes our country safer -- so we will act boldly in freedom's cause.\",\n",
       " '(Applause.)',\n",
       " 'Far from being a hopeless dream, the advance of freedom is the great story of our time.',\n",
       " 'In 1945, there were about two dozen lonely democracies in the world.',\n",
       " 'Today, there are 122.',\n",
       " \"And we're writing a new chapter in the story of self-government -- with women lining up to vote in Afghanistan, and millions of Iraqis marking their liberty with purple ink, and men and women from Lebanon to Egypt debating the rights of individuals and the necessity of freedom.\",\n",
       " 'At the start of 2006, more than half the people of our world live in democratic nations.',\n",
       " 'And we do not forget the other half -- in places like Syria and Burma, Zimbabwe, North Korea, and Iran -- because the demands of justice, and the peace of this world, require their freedom, as well.',\n",
       " '(Applause.)',\n",
       " 'President George W. Bush delivers his State of the Union Address at the Capitol, Tuesday, Jan.',\n",
       " '31, 2006.',\n",
       " 'White House photo by Eric Draper No one can deny the success of freedom, but some men rage and fight against it.',\n",
       " 'And one of the main sources of reaction and opposition is radical Islam -- the perversion by a few of a noble faith into an ideology of terror and death.',\n",
       " 'Terrorists like bin Laden are serious about mass murder -- and all of us must take their declared intentions seriously.',\n",
       " 'They seek to impose a heartless system of totalitarian control throughout the Middle East, and arm themselves with weapons of mass murder.',\n",
       " 'Their aim is to seize power in Iraq, and use it as a safe haven to launch attacks against America and the world.',\n",
       " 'Lacking the military strength to challenge us directly, the terrorists have chosen the weapon of fear.',\n",
       " 'When they murder children at a school in Beslan, or blow up commuters in London, or behead a bound captive, the terrorists hope these horrors will break our will, allowing the violent to inherit the Earth.',\n",
       " 'But they have miscalculated: We love our freedom, and we will fight to keep it.',\n",
       " '(Applause.)',\n",
       " 'In a time of testing, we cannot find security by abandoning our commitments and retreating within our borders.',\n",
       " 'If we were to leave these vicious attackers alone, they would not leave us alone.',\n",
       " 'They would simply move the battlefield to our own shores.',\n",
       " 'There is no peace in retreat.',\n",
       " 'And there is no honor in retreat.',\n",
       " 'By allowing radical Islam to work its will -- by leaving an assaulted world to fend for itself -- we would signal to all that we no longer believe in our own ideals, or even in our own courage.',\n",
       " 'But our enemies and our friends can be certain: The United States will not retreat from the world, and we will never surrender to evil.',\n",
       " '(Applause.)',\n",
       " 'America rejects the false comfort of isolationism.',\n",
       " 'We are the nation that saved liberty in Europe, and liberated death camps, and helped raise up democracies, and faced down an evil empire.',\n",
       " 'Once again, we accept the call of history to deliver the oppressed and move this world toward peace.',\n",
       " 'We remain on the offensive against terror networks.',\n",
       " 'We have killed or captured many of their leaders -- and for the others, their day will come.',\n",
       " 'President George W. Bush greets members of Congress after his State of the Union Address at the Capitol, Tuesday, Jan.',\n",
       " '31, 2006.',\n",
       " 'White House photo by Eric Draper We remain on the offensive in Afghanistan, where a fine President and a National Assembly are fighting terror while building the institutions of a new democracy.',\n",
       " \"We're on the offensive in Iraq, with a clear plan for victory.\",\n",
       " \"First, we're helping Iraqis build an inclusive government, so that old resentments will be eased and the insurgency will be marginalized.\",\n",
       " \"Second, we're continuing reconstruction efforts, and helping the Iraqi government to fight corruption and build a modern economy, so all Iraqis can experience the benefits of freedom.\",\n",
       " \"And, third, we're striking terrorist targets while we train Iraqi forces that are increasingly capable of defeating the enemy.\",\n",
       " 'Iraqis are showing their courage every day, and we are proud to be their allies in the cause of freedom.',\n",
       " '(Applause.)',\n",
       " 'Our work in Iraq is difficult because our enemy is brutal.',\n",
       " 'But that brutality has not stopped the dramatic progress of a new democracy.',\n",
       " 'In less than three years, the nation has gone from dictatorship to liberation, to sovereignty, to a constitution, to national elections.',\n",
       " 'At the same time, our coalition has been relentless in shutting off terrorist infiltration, clearing out insurgent strongholds, and turning over territory to Iraqi security forces.',\n",
       " 'I am confident in our plan for victory; I am confident in the will of the Iraqi people; I am confident in the skill and spirit of our military.',\n",
       " 'Fellow citizens, we are in this fight to win, and we are winning.',\n",
       " '(Applause.)',\n",
       " 'The road of victory is the road that will take our troops home.',\n",
       " 'As we make progress on the ground, and Iraqi forces increasingly take the lead, we should be able to further decrease our troop levels -- but those decisions will be made by our military commanders, not by politicians in Washington, D.C.',\n",
       " '(Applause.)',\n",
       " 'Our coalition has learned from our experience in Iraq.',\n",
       " \"We've adjusted our military tactics and changed our approach to reconstruction.\",\n",
       " 'Along the way, we have benefitted from responsible criticism and counsel offered by members of Congress of both parties.',\n",
       " 'In the coming year, I will continue to reach out and seek your good advice.',\n",
       " 'Yet, there is a difference between responsible criticism that aims for success, and defeatism that refuses to acknowledge anything but failure.',\n",
       " '(Applause.)',\n",
       " 'Hindsight alone is not wisdom, and second-guessing is not a strategy.',\n",
       " '(Applause.)',\n",
       " 'With so much in the balance, those of us in public office have a duty to speak with candor.',\n",
       " 'A sudden withdrawal of our forces from Iraq would abandon our Iraqi allies to death and prison, would put men like bin Laden and Zarqawi in charge of a strategic country, and show that a pledge from America means little.',\n",
       " 'Members of Congress, however we feel about the decisions and debates of the past, our nation has only one option: We must keep our word, defeat our enemies, and stand behind the American military in this vital mission.',\n",
       " '(Applause.)',\n",
       " 'Laura Bush is applauded as she is introduced Tuesday evening, Jan.',\n",
       " '31, 2006 during the State of the Union Address at United States Capitol in Washington.',\n",
       " 'White House photo by Eric Draper Our men and women in uniform are making sacrifices -- and showing a sense of duty stronger than all fear.',\n",
       " \"They know what it's like to fight house to house in a maze of streets, to wear heavy gear in the desert heat, to see a comrade killed by a roadside bomb.\",\n",
       " 'And those who know the costs also know the stakes.',\n",
       " 'Marine Staff Sergeant Dan Clay was killed last month fighting in Fallujah.',\n",
       " 'He left behind a letter to his family, but his words could just as well be addressed to every American.',\n",
       " 'Here is what Dan wrote: \"I know what honor is.',\n",
       " '... It has been an honor to protect and serve all of you.',\n",
       " 'I faced death with the secure knowledge that you would not have to....',\n",
       " 'Never falter!',\n",
       " 'Don\\'t hesitate to honor and support those of us who have the honor of protecting that which is worth protecting.\"',\n",
       " \"Staff Sergeant Dan Clay's wife, Lisa, and his mom and dad, Sara Jo and Bud, are with us this evening.\",\n",
       " 'Welcome.',\n",
       " '(Applause.)',\n",
       " 'Our nation is grateful to the fallen, who live in the memory of our country.',\n",
       " \"We're grateful to all who volunteer to wear our nation's uniform -- and as we honor our brave troops, let us never forget the sacrifices of America's military families.\",\n",
       " '(Applause.)',\n",
       " 'Our offensive against terror involves more than military action.',\n",
       " 'Ultimately, the only way to defeat the terrorists is to defeat their dark vision of hatred and fear by offering the hopeful alternative of political freedom and peaceful change.',\n",
       " 'So the United States of America supports democratic reform across the broader Middle East.',\n",
       " 'Elections are vital, but they are only the beginning.',\n",
       " 'Raising up a democracy requires the rule of law, and protection of minorities, and strong, accountable institutions that last longer than a single vote.',\n",
       " 'The great people of Egypt have voted in a multi-party presidential election -- and now their government should open paths of peaceful opposition that will reduce the appeal of radicalism.',\n",
       " 'The Palestinian people have voted in elections.',\n",
       " 'And now the leaders of Hamas must recognize Israel, disarm, reject terrorism, and work for lasting peace.',\n",
       " '(Applause.)',\n",
       " 'Saudi Arabia has taken the first steps of reform -- now it can offer its people a better future by pressing forward with those efforts.',\n",
       " 'Democracies in the Middle East will not look like our own, because they will reflect the traditions of their own citizens.',\n",
       " 'Yet liberty is the future of every nation in the Middle East, because liberty is the right and hope of all humanity.',\n",
       " '(Applause.)',\n",
       " 'President George W. Bush waves toward the upper visitors gallery of the House Chamber following his State of the Union remarks Tuesday, Jan.',\n",
       " '31, 2006 at the United States Capitol.',\n",
       " 'White House photo by Eric Draper The same is true of Iran, a nation now held hostage by a small clerical elite that is isolating and repressing its people.',\n",
       " 'The regime in that country sponsors terrorists in the Palestinian territories and in Lebanon -- and that must come to an end.',\n",
       " '(Applause.)',\n",
       " 'The Iranian government is defying the world with its nuclear ambitions, and the nations of the world must not permit the Iranian regime to gain nuclear weapons.',\n",
       " '(Applause.)',\n",
       " 'America will continue to rally the world to confront these threats.',\n",
       " 'Tonight, let me speak directly to the citizens of Iran: America respects you, and we respect your country.',\n",
       " 'We respect your right to choose your own future and win your own freedom.',\n",
       " 'And our nation hopes one day to be the closest of friends with a free and democratic Iran.',\n",
       " '(Applause.)',\n",
       " 'To overcome dangers in our world, we must also take the offensive by encouraging economic progress, and fighting disease, and spreading hope in hopeless lands.',\n",
       " 'Isolationism would not only tie our hands in fighting enemies, it would keep us from helping our friends in desperate need.',\n",
       " 'We show compassion abroad because Americans believe in the God-given dignity and worth of a villager with HIV/AIDS, or an infant with malaria, or a refugee fleeing genocide, or a young girl sold into slavery.',\n",
       " 'We also show compassion abroad because regions overwhelmed by poverty, corruption, and despair are sources of terrorism, and organized crime, and human trafficking, and the drug trade.',\n",
       " 'In recent years, you and I have taken unprecedented action to fight AIDS and malaria, expand the education of girls, and reward developing nations that are moving forward with economic and political reform.',\n",
       " 'For people everywhere, the United States is a partner for a better life.',\n",
       " 'Short-changing these efforts would increase the suffering and chaos of our world, undercut our long-term security, and dull the conscience of our country.',\n",
       " 'I urge members of Congress to serve the interests of America by showing the compassion of America.',\n",
       " 'Our country must also remain on the offensive against terrorism here at home.',\n",
       " 'The enemy has not lost the desire or capability to attack us.',\n",
       " 'Fortunately, this nation has superb professionals in law enforcement, intelligence, the military, and homeland security.',\n",
       " 'These men and women are dedicating their lives, protecting us all, and they deserve our support and our thanks.',\n",
       " '(Applause.)',\n",
       " 'They also deserve the same tools they already use to fight drug trafficking and organized crime -- so I ask you to reauthorize the Patriot Act.',\n",
       " '(Applause.)',\n",
       " 'It is said that prior to the attacks of September the 11th, our government failed to connect the dots of the conspiracy.',\n",
       " 'We now know that two of the hijackers in the United States placed telephone calls to al Qaeda operatives overseas.',\n",
       " 'But we did not know about their plans until it was too late.',\n",
       " 'So to prevent another attack -- based on authority given to me by the Constitution and by statute -- I have authorized a terrorist surveillance program to aggressively pursue the international communications of suspected al Qaeda operatives and affiliates to and from America.',\n",
       " 'Previous Presidents have used the same constitutional authority I have, and federal courts have approved the use of that authority.',\n",
       " 'Appropriate members of Congress have been kept informed.',\n",
       " 'The terrorist surveillance program has helped prevent terrorist attacks.',\n",
       " 'It remains essential to the security of America.',\n",
       " 'If there are people inside our country who are talking with al Qaeda, we want to know about it, because we will not sit back and wait to be hit again.',\n",
       " '(Applause.)',\n",
       " 'In all these areas -- from the disruption of terror networks, to victory in Iraq, to the spread of freedom and hope in troubled regions -- we need the support of our friends and allies.',\n",
       " 'To draw that support, we must always be clear in our principles and willing to act.',\n",
       " 'The only alternative to American leadership is a dramatically more dangerous and anxious world.',\n",
       " 'Yet we also choose to lead because it is a privilege to serve the values that gave us birth.',\n",
       " 'American leaders -- from Roosevelt to Truman to Kennedy to Reagan -- rejected isolation and retreat, because they knew that America is always more secure when freedom is on the march.',\n",
       " 'Our own generation is in a long war against a determined enemy -- a war that will be fought by Presidents of both parties, who will need steady bipartisan support from the Congress.',\n",
       " 'And tonight I ask for yours.',\n",
       " 'Together, let us protect our country, support the men and women who defend us, and lead this world toward freedom.',\n",
       " '(Applause.)',\n",
       " 'Here at home, America also has a great opportunity: We will build the prosperity of our country by strengthening our economic leadership in the world.',\n",
       " 'Our economy is healthy and vigorous, and growing faster than other major industrialized nations.',\n",
       " 'In the last two-and-a-half years, America has created 4.6 million new jobs -- more than Japan and the European Union combined.',\n",
       " '(Applause.)',\n",
       " 'Even in the face of higher energy prices and natural disasters, the American people have turned in an economic performance that is the envy of the world.',\n",
       " 'The American economy is preeminent, but we cannot afford to be complacent.',\n",
       " \"In a dynamic world economy, we are seeing new competitors, like China and India, and this creates uncertainty, which makes it easier to feed people's fears.\",\n",
       " \"So we're seeing some old temptations return.\",\n",
       " 'Protectionists want to escape competition, pretending that we can keep our high standard of living while walling off our economy.',\n",
       " 'Others say that the government needs to take a larger role in directing the economy, centralizing more power in Washington and increasing taxes.',\n",
       " 'We hear claims that immigrants are somehow bad for the economy -- even though this economy could not function without them.',\n",
       " '(Applause.)',\n",
       " 'All these are forms of economic retreat, and they lead in the same direction -- toward a stagnant and second-rate economy.',\n",
       " 'Tonight I will set out a better path: an agenda for a nation that competes with confidence; an agenda that will raise standards of living and generate new jobs.',\n",
       " 'Americans should not fear our economic future, because we intend to shape it.',\n",
       " 'Keeping America competitive begins with keeping our economy growing.',\n",
       " 'And our economy grows when Americans have more of their own money to spend, save, and invest.',\n",
       " 'In the last five years, the tax relief you passed has left $880 billion in the hands of American workers, investors, small businesses, and families -- and they have used it to help produce more than four years of uninterrupted economic growth.',\n",
       " '(Applause.)',\n",
       " 'Yet the tax relief is set to expire in the next few years.',\n",
       " 'If we do nothing, American families will face a massive tax increase they do not expect and will not welcome.',\n",
       " 'Because America needs more than a temporary expansion, we need more than temporary tax relief.',\n",
       " 'I urge the Congress to act responsibly, and make the tax cuts permanent.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires us to be good stewards of tax dollars.',\n",
       " \"Every year of my presidency, we've reduced the growth of non-security discretionary spending, and last year you passed bills that cut this spending.\",\n",
       " 'This year my budget will cut it again, and reduce or eliminate more than 140 programs that are performing poorly or not fulfilling essential priorities.',\n",
       " 'By passing these reforms, we will save the American taxpayer another $14 billion next year, and stay on track to cut the deficit in half by 2009.',\n",
       " '(Applause.)',\n",
       " 'I am pleased that members of Congress are working on earmark reform, because the federal budget has too many special interest projects.',\n",
       " '(Applause.)',\n",
       " 'And we can tackle this problem together, if you pass the line-item veto.',\n",
       " '(Applause.)',\n",
       " 'We must also confront the larger challenge of mandatory spending, or entitlements.',\n",
       " \"This year, the first of about 78 million baby boomers turn 60, including two of my Dad's favorite people -- me and President Clinton.\",\n",
       " '(Laughter.)',\n",
       " 'This milestone is more than a personal crisis -- (laughter) -- it is a national challenge.',\n",
       " 'The retirement of the baby boom generation will put unprecedented strains on the federal government.',\n",
       " 'By 2030, spending for Social Security, Medicare and Medicaid alone will be almost 60 percent of the entire federal budget.',\n",
       " 'And that will present future Congresses with impossible choices -- staggering tax increases, immense deficits, or deep cuts in every category of spending.',\n",
       " 'Congress did not act last year on my proposal to save Social Security -- (applause) -- yet the rising cost of entitlements is a problem that is not going away.',\n",
       " '(Applause.)',\n",
       " 'And every year we fail to act, the situation gets worse.',\n",
       " 'So tonight, I ask you to join me in creating a commission to examine the full impact of baby boom retirements on Social Security, Medicare, and Medicaid.',\n",
       " 'This commission should include members of Congress of both parties, and offer bipartisan solutions.',\n",
       " 'We need to put aside partisan politics and work together and get this problem solved.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires us to open more markets for all that Americans make and grow.',\n",
       " 'One out of every five factory jobs in America is related to global trade, and we want people everywhere to buy American.',\n",
       " 'With open markets and a level playing field, no one can out-produce or out-compete the American worker.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires an immigration system that upholds our laws, reflects our values, and serves the interests of our economy.',\n",
       " 'Our nation needs orderly and secure borders.',\n",
       " '(Applause.)',\n",
       " 'To meet this goal, we must have stronger immigration enforcement and border protection.',\n",
       " '(Applause.)',\n",
       " 'And we must have a rational, humane guest worker program that rejects amnesty, allows temporary jobs for people who seek them legally, and reduces smuggling and crime at the border.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires affordable health care.',\n",
       " '(Applause.)',\n",
       " 'Our government has a responsibility to provide health care for the poor and the elderly, and we are meeting that responsibility.',\n",
       " '(Applause.)',\n",
       " 'For all Americans -- for all Americans, we must confront the rising cost of care, strengthen the doctor-patient relationship, and help people afford the insurance coverage they need.',\n",
       " '(Applause.)',\n",
       " 'We will make wider use of electronic records and other health information technology, to help control costs and reduce dangerous medical errors.',\n",
       " 'We will strengthen health savings accounts -- making sure individuals and small business employees can buy insurance with the same advantages that people working for big businesses now get.',\n",
       " '(Applause.)',\n",
       " 'We will do more to make this coverage portable, so workers can switch jobs without having to worry about losing their health insurance.',\n",
       " '(Applause.)',\n",
       " 'And because lawsuits are driving many good doctors out of practice -- leaving women in nearly 1,500 American counties without a single OB/GYN -- I ask the Congress to pass medical liability reform this year.',\n",
       " '(Applause.)',\n",
       " 'Keeping America competitive requires affordable energy.',\n",
       " 'And here we have a serious problem: America is addicted to oil, which is often imported from unstable parts of the world.',\n",
       " 'The best way to break this addiction is through technology.',\n",
       " 'Since 2001, we have spent nearly $10 billion to develop cleaner, cheaper, and more reliable alternative energy sources -- and we are on the threshold of incredible advances.',\n",
       " 'So tonight, I announce the Advanced Energy Initiative -- a 22-percent increase in clean-energy research -- at the Department of Energy, to push for breakthroughs in two vital areas.',\n",
       " 'To change how we power our homes and offices, we will invest more in zero-emission coal-fired plants, revolutionary solar and wind technologies, and clean, safe nuclear energy.',\n",
       " '(Applause.)',\n",
       " 'We must also change how we power our automobiles.',\n",
       " 'We will increase our research in better batteries for hybrid and electric cars, and in pollution-free cars that run on hydrogen.',\n",
       " \"We'll also fund additional research in cutting-edge methods of producing ethanol, not just from corn, but from wood chips and stalks, or switch grass.\",\n",
       " 'Our goal is to make this new kind of ethanol practical and competitive within six years.',\n",
       " '(Applause.)',\n",
       " 'Breakthroughs on this and other new technologies will help us reach another great goal: to replace more than 75 percent of our oil imports from the Middle East by 2025.',\n",
       " '(Applause.)',\n",
       " 'By applying the talent and technology of America, this country can dramatically improve our environment, move beyond a petroleum-based economy, and make our dependence on Middle Eastern oil a thing of the past.',\n",
       " '(Applause.)',\n",
       " 'And to keep America competitive, one commitment is necessary above all: We must continue to lead the world in human talent and creativity.',\n",
       " \"Our greatest advantage in the world has always been our educated, hardworking, ambitious people -- and we're going to keep that edge.\",\n",
       " \"Tonight I announce an American Competitiveness Initiative, to encourage innovation throughout our economy, and to give our nation's children a firm grounding in math and science.\",\n",
       " '(Applause.)',\n",
       " 'First, I propose to double the federal commitment to the most critical basic research programs in the physical sciences over the next 10 years.',\n",
       " \"This funding will support the work of America's most creative minds as they explore promising areas such as nanotechnology, supercomputing, and alternative energy sources.\",\n",
       " 'Second, I propose to make permanent the research and development tax credit -- (applause) -- to encourage bolder private-sector initiatives in technology.',\n",
       " 'With more research in both the public and private sectors, we will improve our quality of life -- and ensure that America will lead the world in opportunity and innovation for decades to come.',\n",
       " '(Applause.)',\n",
       " 'Third, we need to encourage children to take more math and science, and to make sure those courses are rigorous enough to compete with other nations.',\n",
       " \"We've made a good start in the early grades with the No Child Left Behind Act, which is raising standards and lifting test scores across our country.\",\n",
       " 'Tonight I propose to train 70,000 high school teachers to lead advanced-placement courses in math and science, bring 30,000 math and science professionals to teach in classrooms, and give early help to students who struggle with math, so they have a better chance at good, high-wage jobs.',\n",
       " \"If we ensure that America's children succeed in life, they will ensure that America succeeds in the world.\",\n",
       " '(Applause.)',\n",
       " 'Preparing our nation to compete in the world is a goal that all of us can share.',\n",
       " 'I urge you to support the American Competitiveness Initiative, and together we will show the world what the American people can achieve.',\n",
       " 'America is a great force for freedom and prosperity.',\n",
       " 'Yet our greatness is not measured in power or luxuries, but by who we are and how we treat one another.',\n",
       " 'So we strive to be a compassionate, decent, hopeful society.',\n",
       " 'In recent years, America has become a more hopeful nation.',\n",
       " 'Violent crime rates have fallen to their lowest levels since the 1970s.',\n",
       " 'Welfare cases have dropped by more than half over the past decade.',\n",
       " 'Drug use among youth is down 19 percent since 2001.',\n",
       " 'There are fewer abortions in America than at any point in the last three decades, and the number of children born to teenage mothers has been falling for a dozen years in a row.',\n",
       " '(Applause.)',\n",
       " 'These gains are evidence of a quiet transformation -- a revolution of conscience, in which a rising generation is finding that a life of personal responsibility is a life of fulfillment.',\n",
       " 'Government has played a role.',\n",
       " 'Wise policies, such as welfare reform and drug education and support for abstinence and adoption have made a difference in the character of our country.',\n",
       " 'And everyone here tonight, Democrat and Republican, has a right to be proud of this record.',\n",
       " '(Applause.)',\n",
       " 'Yet many Americans, especially parents, still have deep concerns about the direction of our culture, and the health of our most basic institutions.',\n",
       " \"They're concerned about unethical conduct by public officials, and discouraged by activist courts that try to redefine marriage.\",\n",
       " 'They worry about children in our society who need direction and love, and about fellow citizens still displaced by natural disaster, and about suffering caused by treatable diseases.',\n",
       " 'As we look at these challenges, we must never give in to the belief that America is in decline, or that our culture is doomed to unravel.',\n",
       " 'The American people know better than that.',\n",
       " 'We have proven the pessimists wrong before -- and we will do it again.',\n",
       " '(Applause.)',\n",
       " 'A hopeful society depends on courts that deliver equal justice under the law.',\n",
       " 'The Supreme Court now has two superb new members -- new members on its bench: Chief Justice John Roberts and Justice Sam Alito.',\n",
       " '(Applause.)',\n",
       " 'I thank the Senate for confirming both of them.',\n",
       " 'I will continue to nominate men and women who understand that judges must be servants of the law, and not legislate from the bench.',\n",
       " '(Applause.)',\n",
       " 'Today marks the official retirement of a very special American.',\n",
       " \"For 24 years of faithful service to our nation, the United States is grateful to Justice Sandra Day O'Connor.\",\n",
       " '(Applause.)',\n",
       " 'A hopeful society has institutions of science and medicine that do not cut ethical corners, and that recognize the matchless value of every life.',\n",
       " 'Tonight I ask you to pass legislation to prohibit the most egregious abuses of medical research: human cloning in all its forms, creating or implanting embryos for experiments, creating human-animal hybrids, and buying, selling, or patenting human embryos.',\n",
       " 'Human life is a gift from our Creator -- and that gift should never be discarded, devalued or put up for sale.',\n",
       " '(Applause.)',\n",
       " 'A hopeful society expects elected officials to uphold the public trust.',\n",
       " '(Applause.)',\n",
       " 'Honorable people in both parties are working on reforms to strengthen the ethical standards of Washington -- I support your efforts.',\n",
       " 'Each of us has made a pledge to be worthy of public responsibility -- and that is a pledge we must never forget, never dismiss, and never betray.',\n",
       " '(Applause.)',\n",
       " 'As we renew the promise of our institutions, let us also show the character of America in our compassion and care for one another.',\n",
       " 'A hopeful society gives special attention to children who lack direction and love.',\n",
       " \"Through the Helping America's Youth Initiative, we are encouraging caring adults to get involved in the life of a child -- and this good work is being led by our First Lady, Laura Bush.\",\n",
       " '(Applause.)',\n",
       " \"This year we will add resources to encourage young people to stay in school, so more of America's youth can raise their sights and achieve their dreams.\",\n",
       " \"A hopeful society comes to the aid of fellow citizens in times of suffering and emergency -- and stays at it until they're back on their feet.\",\n",
       " 'So far the federal government has committed $85 billion to the people of the Gulf Coast and New Orleans.',\n",
       " \"We're removing debris and repairing highways and rebuilding stronger levees.\",\n",
       " \"We're providing business loans and housing assistance.\",\n",
       " 'Yet as we meet these immediate needs, we must also address deeper challenges that existed before the storm arrived.',\n",
       " 'In New Orleans and in other places, many of our fellow citizens have felt excluded from the promise of our country.',\n",
       " 'The answer is not only temporary relief, but schools that teach every child, and job skills that bring upward mobility, and more opportunities to own a home and start a business.',\n",
       " 'As we recover from a disaster, let us also work for the day when all Americans are protected by justice, equal in hope, and rich in opportunity.',\n",
       " '(Applause.)',\n",
       " 'A hopeful society acts boldly to fight diseases like HIV/AIDS, which can be prevented, and treated, and defeated.',\n",
       " 'More than a million Americans live with HIV, and half of all AIDS cases occur among African Americans.',\n",
       " 'I ask Congress to reform and reauthorize the Ryan White Act, and provide new funding to states, so we end the waiting lists for AIDS medicines in America.',\n",
       " '(Applause.)',\n",
       " 'We will also lead a nationwide effort, working closely with African American churches and faith-based groups, to deliver rapid HIV tests to millions, end the stigma of AIDS, and come closer to the day when there are no new infections in America.',\n",
       " '(Applause.)',\n",
       " \"Fellow citizens, we've been called to leadership in a period of consequence.\",\n",
       " \"We've entered a great ideological conflict we did nothing to invite.\",\n",
       " 'We see great changes in science and commerce that will influence all our lives.',\n",
       " 'Sometimes it can seem that history is turning in a wide arc, toward an unknown shore.',\n",
       " 'Yet the destination of history is determined by human action, and every great movement of history comes to a point of choosing.',\n",
       " 'Lincoln could have accepted peace at the cost of disunity and continued slavery.',\n",
       " 'Martin Luther King could have stopped at Birmingham or at Selma, and achieved only half a victory over segregation.',\n",
       " 'The United States could have accepted the permanent division of Europe, and been complicit in the oppression of others.',\n",
       " 'Today, having come far in our own historical journey, we must decide: Will we turn back, or finish well?',\n",
       " 'Before history is written down in books, it is written in courage.',\n",
       " 'Like Americans before us, we will show that courage and we will finish well.',\n",
       " \"We will lead freedom's advance.\",\n",
       " 'We will compete and excel in the global economy.',\n",
       " 'We will renew the defining moral commitments of this land.',\n",
       " 'And so we move forward -- optimistic about our country, faithful to its cause, and confident of the victories to come.',\n",
       " 'May God bless America.',\n",
       " '(Applause.)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "import nltk.draw\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(S\n",
      "  (Chunk Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  (Chunk Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Supreme/NNP Court/NNP)\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  (Chunk called/VBD America/NNP)\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(S\n",
      "  Tonight/NN\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  comforted/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  glad/JJ\n",
      "  reunion/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ\n",
      "  for/IN\n",
      "  the/DT\n",
      "  good/JJ\n",
      "  life/NN\n",
      "  of/IN\n",
      "  (Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "  ./.)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(Chunk Applause/NNP)\n",
      "(S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  reacts/VBZ\n",
      "  to/TO\n",
      "  applause/VB\n",
      "  during/IN\n",
      "  his/PRP$\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(S 31/CD ,/, 2006/CD ./.)\n",
      "(S\n",
      "  (Chunk White/NNP House/NNP photo/NN)\n",
      "  by/IN\n",
      "  (Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  invited/JJ\n",
      "  to/TO\n",
      "  this/DT\n",
      "  rostrum/NN\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  'm/VBP\n",
      "  humbled/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  privilege/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  mindful/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  history/NN\n",
      "  we/PRP\n",
      "  've/VBP\n",
      "  seen/VBN\n",
      "  together/RB\n",
      "  ./.)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "(S\n",
      "  We/PRP\n",
      "  have/VBP\n",
      "  gathered/VBN\n",
      "  under/IN\n",
      "  this/DT\n",
      "  (Chunk Capitol/NNP dome/NN)\n",
      "  in/IN\n",
      "  moments/NNS\n",
      "  of/IN\n",
      "  national/JJ\n",
      "  mourning/NN\n",
      "  and/CC\n",
      "  national/JJ\n",
      "  achievement/NN\n",
      "  ./.)\n",
      "(Chunk Capitol/NNP dome/NN)\n",
      "(S\n",
      "  We/PRP\n",
      "  (Chunk have/VBP served/VBN America/NNP)\n",
      "  through/IN\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  most/RBS\n",
      "  consequential/JJ\n",
      "  periods/NNS\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  history/NN\n",
      "  --/:\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  my/PRP$\n",
      "  honor/NN\n",
      "  to/TO\n",
      "  serve/VB\n",
      "  with/IN\n",
      "  you/PRP\n",
      "  ./.)\n",
      "(Chunk have/VBP served/VBN America/NNP)\n",
      "(S\n",
      "  In/IN\n",
      "  a/DT\n",
      "  system/NN\n",
      "  of/IN\n",
      "  two/CD\n",
      "  parties/NNS\n",
      "  ,/,\n",
      "  two/CD\n",
      "  chambers/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  two/CD\n",
      "  elected/JJ\n",
      "  branches/NNS\n",
      "  ,/,\n",
      "  there/EX\n",
      "  will/MD\n",
      "  always/RB\n",
      "  be/VB\n",
      "  differences/NNS\n",
      "  and/CC\n",
      "  debate/NN\n",
      "  ./.)\n",
      "(S\n",
      "  But/CC\n",
      "  even/RB\n",
      "  tough/JJ\n",
      "  debates/NNS\n",
      "  can/MD\n",
      "  be/VB\n",
      "  conducted/VBN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  civil/JJ\n",
      "  tone/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  our/PRP$\n",
      "  differences/NNS\n",
      "  can/MD\n",
      "  not/RB\n",
      "  be/VB\n",
      "  allowed/VBN\n",
      "  to/TO\n",
      "  harden/VB\n",
      "  into/IN\n",
      "  anger/NN\n",
      "  ./.)\n",
      "(S\n",
      "  To/TO\n",
      "  confront/VB\n",
      "  the/DT\n",
      "  great/JJ\n",
      "  issues/NNS\n",
      "  before/IN\n",
      "  us/PRP\n",
      "  ,/,\n",
      "  we/PRP\n",
      "  must/MD\n",
      "  act/VB\n",
      "  in/IN\n",
      "  a/DT\n",
      "  spirit/NN\n",
      "  of/IN\n",
      "  goodwill/NN\n",
      "  and/CC\n",
      "  respect/NN\n",
      "  for/IN\n",
      "  one/CD\n",
      "  another/DT\n",
      "  --/:\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  will/MD\n",
      "  do/VB\n",
      "  my/PRP$\n",
      "  part/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Tonight/NNP)\n",
      "  the/DT\n",
      "  state/NN\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  (Chunk Union/NNP)\n",
      "  is/VBZ\n",
      "  strong/JJ\n",
      "  --/:\n",
      "  and/CC\n",
      "  together/RB\n",
      "  we/PRP\n",
      "  will/MD\n",
      "  make/VB\n",
      "  it/PRP\n",
      "  stronger/JJR\n",
      "  ./.)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Union/NNP)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(Chunk Applause/NNP)\n",
      "(S\n",
      "  In/IN\n",
      "  this/DT\n",
      "  decisive/JJ\n",
      "  year/NN\n",
      "  ,/,\n",
      "  you/PRP\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  will/MD\n",
      "  make/VB\n",
      "  choices/NNS\n",
      "  that/WDT\n",
      "  determine/VBP\n",
      "  both/DT\n",
      "  the/DT\n",
      "  future/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  character/NN\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  country/NN\n",
      "  ./.)\n",
      "(S\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  choose/VB\n",
      "  to/TO\n",
      "  act/VB\n",
      "  confidently/RB\n",
      "  in/IN\n",
      "  pursuing/VBG\n",
      "  the/DT\n",
      "  enemies/NNS\n",
      "  of/IN\n",
      "  freedom/NN\n",
      "  --/:\n",
      "  or/CC\n",
      "  retreat/NN\n",
      "  from/IN\n",
      "  our/PRP$\n",
      "  duties/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  hope/NN\n",
      "  of/IN\n",
      "  an/DT\n",
      "  easier/JJR\n",
      "  life/NN\n",
      "  ./.)\n",
      "(S\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  choose/VB\n",
      "  to/TO\n",
      "  build/VB\n",
      "  our/PRP$\n",
      "  prosperity/NN\n",
      "  by/IN\n",
      "  leading/VBG\n",
      "  the/DT\n",
      "  world/NN\n",
      "  economy/NN\n",
      "  --/:\n",
      "  or/CC\n",
      "  shut/VB\n",
      "  ourselves/PRP\n",
      "  off/RP\n",
      "  from/IN\n",
      "  trade/NN\n",
      "  and/CC\n",
      "  opportunity/NN\n",
      "  ./.)\n",
      "(S\n",
      "  In/IN\n",
      "  a/DT\n",
      "  complex/JJ\n",
      "  and/CC\n",
      "  challenging/JJ\n",
      "  time/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  road/NN\n",
      "  of/IN\n",
      "  isolationism/NN\n",
      "  and/CC\n",
      "  protectionism/NN\n",
      "  may/MD\n",
      "  seem/VB\n",
      "  broad/JJ\n",
      "  and/CC\n",
      "  inviting/NN\n",
      "  --/:\n",
      "  yet/CC\n",
      "  it/PRP\n",
      "  ends/VBZ\n",
      "  in/IN\n",
      "  danger/NN\n",
      "  and/CC\n",
      "  decline/NN\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  only/JJ\n",
      "  way/NN\n",
      "  to/TO\n",
      "  protect/VB\n",
      "  our/PRP$\n",
      "  people/NNS\n",
      "  ,/,\n",
      "  the/DT\n",
      "  only/JJ\n",
      "  way/NN\n",
      "  to/TO\n",
      "  secure/VB\n",
      "  the/DT\n",
      "  peace/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  only/JJ\n",
      "  way/NN\n",
      "  to/TO\n",
      "  control/VB\n",
      "  our/PRP$\n",
      "  destiny/NN\n",
      "  is/VBZ\n",
      "  by/IN\n",
      "  our/PRP$\n",
      "  leadership/NN\n",
      "  --/:\n",
      "  so/IN\n",
      "  the/DT\n",
      "  (Chunk United/NNP)\n",
      "  States/NNPS\n",
      "  of/IN\n",
      "  (Chunk America/NNP)\n",
      "  will/MD\n",
      "  continue/VB\n",
      "  to/TO\n",
      "  lead/VB\n",
      "  ./.)\n",
      "(Chunk United/NNP)\n",
      "(Chunk America/NNP)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(Chunk Applause/NNP)\n",
      "(S\n",
      "  Abroad/RB\n",
      "  ,/,\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  is/VBZ\n",
      "  committed/VBN\n",
      "  to/TO\n",
      "  an/DT\n",
      "  historic/JJ\n",
      "  ,/,\n",
      "  long-term/JJ\n",
      "  goal/NN\n",
      "  --/:\n",
      "  we/PRP\n",
      "  seek/VBP\n",
      "  the/DT\n",
      "  end/NN\n",
      "  of/IN\n",
      "  tyranny/NN\n",
      "  in/IN\n",
      "  our/PRP$\n",
      "  world/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Some/DT\n",
      "  dismiss/VBP\n",
      "  that/DT\n",
      "  goal/NN\n",
      "  as/IN\n",
      "  misguided/JJ\n",
      "  idealism/NN\n",
      "  ./.)\n",
      "(S\n",
      "  In/IN\n",
      "  reality/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  future/JJ\n",
      "  security/NN\n",
      "  of/IN\n",
      "  (Chunk America/NNP)\n",
      "  depends/VBZ\n",
      "  on/IN\n",
      "  it/PRP\n",
      "  ./.)\n",
      "(Chunk America/NNP)\n",
      "(S\n",
      "  On/IN\n",
      "  (Chunk September/NNP)\n",
      "  the/DT\n",
      "  11th/CD\n",
      "  ,/,\n",
      "  2001/CD\n",
      "  ,/,\n",
      "  we/PRP\n",
      "  found/VBD\n",
      "  that/IN\n",
      "  problems/NNS\n",
      "  originating/VBG\n",
      "  in/IN\n",
      "  a/DT\n",
      "  failed/JJ\n",
      "  and/CC\n",
      "  oppressive/JJ\n",
      "  state/NN\n",
      "  7,000/CD\n",
      "  miles/NNS\n",
      "  away/RB\n",
      "  could/MD\n",
      "  bring/VB\n",
      "  murder/NN\n",
      "  and/CC\n",
      "  destruction/NN\n",
      "  to/TO\n",
      "  our/PRP$\n",
      "  country/NN\n",
      "  ./.)\n",
      "(Chunk September/NNP)\n",
      "(S\n",
      "  (Chunk Dictatorships/NNP shelter/NN)\n",
      "  terrorists/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  feed/VB\n",
      "  resentment/NN\n",
      "  and/CC\n",
      "  radicalism/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  seek/JJ\n",
      "  weapons/NNS\n",
      "  of/IN\n",
      "  mass/NN\n",
      "  destruction/NN\n",
      "  ./.)\n",
      "(Chunk Dictatorships/NNP shelter/NN)\n",
      "(S\n",
      "  Democracies/NNS\n",
      "  replace/VB\n",
      "  resentment/NN\n",
      "  with/IN\n",
      "  hope/NN\n",
      "  ,/,\n",
      "  respect/VB\n",
      "  the/DT\n",
      "  rights/NNS\n",
      "  of/IN\n",
      "  their/PRP$\n",
      "  citizens/NNS\n",
      "  and/CC\n",
      "  their/PRP$\n",
      "  neighbors/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  join/VB\n",
      "  the/DT\n",
      "  fight/NN\n",
      "  against/IN\n",
      "  terror/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Every/DT\n",
      "  step/NN\n",
      "  toward/IN\n",
      "  freedom/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  world/NN\n",
      "  makes/VBZ\n",
      "  our/PRP$\n",
      "  country/NN\n",
      "  safer/NN\n",
      "  --/:\n",
      "  so/IN\n",
      "  we/PRP\n",
      "  will/MD\n",
      "  act/VB\n",
      "  boldly/RB\n",
      "  in/IN\n",
      "  freedom/NN\n",
      "  's/POS\n",
      "  cause/NN\n",
      "  ./.)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(Chunk Applause/NNP)\n",
      "(S\n",
      "  Far/RB\n",
      "  from/IN\n",
      "  being/VBG\n",
      "  a/DT\n",
      "  hopeless/NN\n",
      "  dream/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  advance/NN\n",
      "  of/IN\n",
      "  freedom/NN\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  great/JJ\n",
      "  story/NN\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  time/NN\n",
      "  ./.)\n",
      "(S\n",
      "  In/IN\n",
      "  1945/CD\n",
      "  ,/,\n",
      "  there/EX\n",
      "  were/VBD\n",
      "  about/RB\n",
      "  two/CD\n",
      "  dozen/NN\n",
      "  lonely/RB\n",
      "  democracies/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  world/NN\n",
      "  ./.)\n",
      "(S Today/NN ,/, there/EX are/VBP 122/CD ./.)\n",
      "(S\n",
      "  And/CC\n",
      "  we/PRP\n",
      "  're/VBP\n",
      "  writing/VBG\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  chapter/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  story/NN\n",
      "  of/IN\n",
      "  self-government/JJ\n",
      "  --/:\n",
      "  with/IN\n",
      "  women/NNS\n",
      "  lining/VBG\n",
      "  up/RP\n",
      "  to/TO\n",
      "  vote/VB\n",
      "  in/IN\n",
      "  (Chunk Afghanistan/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  millions/NNS\n",
      "  of/IN\n",
      "  (Chunk Iraqis/NNP)\n",
      "  marking/VBG\n",
      "  their/PRP$\n",
      "  liberty/NN\n",
      "  with/IN\n",
      "  purple/JJ\n",
      "  ink/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  men/NNS\n",
      "  and/CC\n",
      "  women/NNS\n",
      "  from/IN\n",
      "  (Chunk Lebanon/NNP)\n",
      "  to/TO\n",
      "  (Chunk Egypt/NNP)\n",
      "  debating/VBG\n",
      "  the/DT\n",
      "  rights/NNS\n",
      "  of/IN\n",
      "  individuals/NNS\n",
      "  and/CC\n",
      "  the/DT\n",
      "  necessity/NN\n",
      "  of/IN\n",
      "  freedom/NN\n",
      "  ./.)\n",
      "(Chunk Afghanistan/NNP)\n",
      "(Chunk Iraqis/NNP)\n",
      "(Chunk Lebanon/NNP)\n",
      "(Chunk Egypt/NNP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  At/IN\n",
      "  the/DT\n",
      "  start/NN\n",
      "  of/IN\n",
      "  2006/CD\n",
      "  ,/,\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  half/PDT\n",
      "  the/DT\n",
      "  people/NNS\n",
      "  of/IN\n",
      "  our/PRP$\n",
      "  world/NN\n",
      "  live/VBP\n",
      "  in/IN\n",
      "  democratic/JJ\n",
      "  nations/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  And/CC\n",
      "  we/PRP\n",
      "  do/VBP\n",
      "  not/RB\n",
      "  forget/VB\n",
      "  the/DT\n",
      "  other/JJ\n",
      "  half/NN\n",
      "  --/:\n",
      "  in/IN\n",
      "  places/NNS\n",
      "  like/IN\n",
      "  (Chunk Syria/NNP)\n",
      "  and/CC\n",
      "  (Chunk Burma/NNP)\n",
      "  ,/,\n",
      "  (Chunk Zimbabwe/NNP)\n",
      "  ,/,\n",
      "  (Chunk North/NNP Korea/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (Chunk Iran/NNP)\n",
      "  --/:\n",
      "  because/IN\n",
      "  the/DT\n",
      "  demands/NNS\n",
      "  of/IN\n",
      "  justice/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  the/DT\n",
      "  peace/NN\n",
      "  of/IN\n",
      "  this/DT\n",
      "  world/NN\n",
      "  ,/,\n",
      "  require/VBP\n",
      "  their/PRP$\n",
      "  freedom/NN\n",
      "  ,/,\n",
      "  as/RB\n",
      "  well/RB\n",
      "  ./.)\n",
      "(Chunk Syria/NNP)\n",
      "(Chunk Burma/NNP)\n",
      "(Chunk Zimbabwe/NNP)\n",
      "(Chunk North/NNP Korea/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(S (/( (Chunk Applause/NNP) ./. )/))\n",
      "(Chunk Applause/NNP)\n",
      "(S\n",
      "  (Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "  delivers/VBZ\n",
      "  his/PRP$\n",
      "  State/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Chunk Union/NNP Address/NNP)\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (Chunk Capitol/NNP)\n",
      "  ,/,\n",
      "  (Chunk Tuesday/NNP)\n",
      "  ,/,\n",
      "  (Chunk Jan/NNP)\n",
      "  ./.)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7497a9a03639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprocess_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7497a9a03639>\u001b[0m in \u001b[0;36mprocess_content\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mchunked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \"\"\"\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \"\"\"\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns[0].name())\n",
    "print(syns[0].lemmas()[0].name())\n",
    "print(syns[0].definition())\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'just', 'thoroughly', 'expert', 'honest', 'good', 'adept', 'sound', 'practiced', 'salutary', 'secure', 'beneficial', 'honorable', 'skilful', 'estimable', 'skillful', 'respectable', 'right', 'undecomposed', 'dear', 'trade_good', 'in_effect', 'well', 'unspoiled', 'in_force', 'safe', 'proficient', 'unspoilt', 'upright', 'full', 'near', 'goodness', 'ripe', 'effective', 'soundly', 'serious', 'dependable', 'commodity'}\n",
      "{'evilness', 'badness', 'evil', 'ill', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(documents[1])\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(documents)\n",
    "all_words = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set that we'll train our classifier with\n",
    "training_set = featuresets[:1900]\n",
    "\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 76.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              schumacher = True              neg : pos    =     12.3 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.2 : 1.0\n",
      "                  justin = True              neg : pos    =      9.6 : 1.0\n",
      "                  annual = True              pos : neg    =      9.0 : 1.0\n",
      "                 frances = True              pos : neg    =      8.4 : 1.0\n",
      "           unimaginative = True              neg : pos    =      8.3 : 1.0\n",
      "                 martian = True              neg : pos    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.2 : 1.0\n",
      "                  suvari = True              neg : pos    =      7.0 : 1.0\n",
      "               atrocious = True              neg : pos    =      7.0 : 1.0\n",
      "                    mena = True              neg : pos    =      7.0 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.0 : 1.0\n",
      "                 singers = True              pos : neg    =      6.3 : 1.0\n",
      "                 cunning = True              pos : neg    =      6.3 : 1.0\n",
      "                obstacle = True              pos : neg    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB in BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy percent: 0.75\n",
      "BernoulliNB accuracy percent: 0.75\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MultinomialNB accuracy percent:\",nltk.classify.accuracy(MNB_classifier, testing_set))\n",
    "\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB accuracy percent:\",nltk.classify.accuracy(BNB_classifier, testing_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearni modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 76.0\n",
      "Most Informative Features\n",
      "              schumacher = True              neg : pos    =     12.3 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.2 : 1.0\n",
      "                  justin = True              neg : pos    =      9.6 : 1.0\n",
      "                  annual = True              pos : neg    =      9.0 : 1.0\n",
      "                 frances = True              pos : neg    =      8.4 : 1.0\n",
      "           unimaginative = True              neg : pos    =      8.3 : 1.0\n",
      "                 martian = True              neg : pos    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.2 : 1.0\n",
      "                  suvari = True              neg : pos    =      7.0 : 1.0\n",
      "               atrocious = True              neg : pos    =      7.0 : 1.0\n",
      "                    mena = True              neg : pos    =      7.0 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.0 : 1.0\n",
      "                 singers = True              pos : neg    =      6.3 : 1.0\n",
      "                 cunning = True              pos : neg    =      6.3 : 1.0\n",
      "                obstacle = True              pos : neg    =      6.3 : 1.0\n",
      "MNB_classifier accuracy percent: 75.0\n",
      "BernoulliNB_classifier accuracy percent: 75.0\n",
      "LogisticRegression_classifier accuracy percent: 78.0\n",
      "SGDClassifier_classifier accuracy percent: 79.0\n",
      "SVC_classifier accuracy percent: 75.0\n",
      "LinearSVC_classifier accuracy percent: 77.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ansambli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
